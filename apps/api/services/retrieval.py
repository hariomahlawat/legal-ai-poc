from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import json
import pickle
import re
import threading


# ----------------------------
# Offline Hybrid Retrieval (BM25 + FAISS)
# ----------------------------
# This module is intentionally dependency-tolerant:
# - If FAISS / sentence-transformers are unavailable, it falls back to BM25 only.
# - If BM25 is unavailable, it returns an empty list (caller should handle).
#
# Artifacts expected (generated by scripts/ingest.py):
#   data/embeddings/bm25.pkl
#   data/embeddings/bm25_meta.jsonl
#   data/embeddings/faiss.index
#   data/embeddings/faiss_meta.jsonl
#
# Meta JSONL schema per line:
#   {"i": int, "chunk_id": str, "source_file": str, "heading_path": str, "text": str}
#
# Returned citations must include FULL verbatim text so synthesis can be grounded.
# Contract per item:
# {
#   "citation_id": "...",                # stable id used by UI and answer brackets
#   "document": "MML",
#   "title": "...",
#   "source_file": "...",
#   "heading": "...",
#   "location": "...",
#   "verbatim": "...",
#   "context_before": "...",
#   "context_after": "...",
#   "snippet": "...",
# }


@dataclass(frozen=True)
class _Chunk:
    i: int
    chunk_id: str
    source_file: str
    heading_path: str
    text: str


_TOKEN_RE = re.compile(r"[A-Za-z0-9]+(?:[-/][A-Za-z0-9]+)*")


def _bm25_tokenize(text: str) -> List[str]:
    # Match scripts/ingest.py tokenization for consistency.
    return [t.lower() for t in _TOKEN_RE.findall(text)]


def _repo_root() -> Path:
    # apps/api/services/retrieval.py -> repo root is parents[3]
    return Path(__file__).resolve().parents[3]


def _short_heading(heading_path: str) -> str:
    # heading_path format: "## CHAPTER X > ### SUBHEAD"
    parts = [p.strip() for p in heading_path.split(">") if p.strip()]
    return parts[-1] if parts else heading_path.strip()


def _soft_intent_boost(legal_object: Optional[str], chunk: _Chunk) -> float:
    """
    A small scoring boost (or penalty) to keep retrieval on the correct legal object
    without hard-filtering away potentially relevant material.
    """
    if not legal_object:
        return 0.0

    hp = (chunk.heading_path or "").lower()
    txt = (chunk.text or "").lower()

    if legal_object == "Court of Inquiry":
        # Strong positive if evidence explicitly mentions COI
        if "court of inquiry" in hp or "court of inquiry" in txt:
            return 0.20
        # Penalize court-martial dominant content unless COI appears too
        if ("court-martial" in hp or "court-martial" in txt) and "court of inquiry" not in txt:
            return -0.15
        return 0.0

    if legal_object == "Court-Martial":
        if "court-martial" in hp or "court-martial" in txt or "courts-martial" in txt:
            return 0.20
        if "court of inquiry" in hp or "court of inquiry" in txt:
            return -0.10
        return 0.0

    if legal_object == "Disciplinary Action":
        if any(k in txt for k in ("awl", "awol", "absence without leave", "desertion")):
            return 0.15
        return 0.0

    return 0.0


class _RetrievalState:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self.loaded = False

        self.bm25 = None
        self.bm25_meta: List[_Chunk] = []

        self.faiss_index = None
        self.faiss_meta: List[_Chunk] = []
        self.embed_model = None

        self.load_errors: List[str] = []


_STATE = _RetrievalState()


def _load_meta_jsonl(path: Path) -> List[_Chunk]:
    chunks: List[_Chunk] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            chunks.append(
                _Chunk(
                    i=int(obj["i"]),
                    chunk_id=str(obj["chunk_id"]),
                    source_file=str(obj["source_file"]),
                    heading_path=str(obj.get("heading_path", "")),
                    text=str(obj.get("text", "")),
                )
            )
    return chunks


def _ensure_loaded() -> None:
    if _STATE.loaded:
        return
    with _STATE._lock:
        if _STATE.loaded:
            return

        root = _repo_root()
        emb_dir = root / "data" / "embeddings"

        # BM25 (required)
        try:
            with (emb_dir / "bm25.pkl").open("rb") as f:
                _STATE.bm25 = pickle.load(f)
            _STATE.bm25_meta = _load_meta_jsonl(emb_dir / "bm25_meta.jsonl")
        except Exception as e:
            _STATE.load_errors.append(f"BM25 load failed: {e}")
            _STATE.bm25 = None
            _STATE.bm25_meta = []

        # FAISS + embeddings (optional)
        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer  # type: ignore

            _STATE.faiss_index = faiss.read_index(str(emb_dir / "faiss.index"))
            _STATE.faiss_meta = _load_meta_jsonl(emb_dir / "faiss_meta.jsonl")

            # Match scripts/ingest.py model
            model_name = "sentence-transformers/all-MiniLM-L6-v2"
            _STATE.embed_model = SentenceTransformer(model_name)
        except Exception as e:
            _STATE.load_errors.append(f"FAISS load failed (fallback to BM25-only): {e}")
            _STATE.faiss_index = None
            _STATE.faiss_meta = []
            _STATE.embed_model = None

        _STATE.loaded = True


def _normalize_scores(pairs: List[Tuple[int, float]]) -> Dict[int, float]:
    if not pairs:
        return {}
    scores = [s for _, s in pairs]
    hi = max(scores)
    lo = min(scores)
    out: Dict[int, float] = {}
    if hi == lo:
        for i, _s in pairs:
            out[i] = 1.0
        return out
    for i, s in pairs:
        out[i] = (s - lo) / (hi - lo)
    return out


# ----------------------------
# Single-query hybrid retrieval
# ----------------------------
def retrieve_citations(
    question: str,
    top_k: int = 8,
    legal_object: Optional[str] = None,
    lexical_k: int = 60,
    semantic_k: int = 60,
) -> List[Dict[str, Any]]:
    """
    Hybrid retrieval over MML chunks.

    - legal_object: optional intent constraint used for soft scoring.
    - Returns top_k citations with full verbatim text and minimal context.
    """
    _ensure_loaded()

    q = (question or "").strip()
    if not q:
        return []

    # If BM25 is unavailable, we cannot retrieve reliably
    if _STATE.bm25 is None or not _STATE.bm25_meta:
        return []

    # 1) Lexical retrieval (BM25)
    q_tokens = _bm25_tokenize(q)
    try:
        bm25_scores = _STATE.bm25.get_scores(q_tokens)  # type: ignore[attr-defined]
        # top lexical_k indices by score
        bm25_pairs = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:lexical_k]
    except Exception:
        bm25_pairs = []

    bm25_norm = _normalize_scores([(i, float(s)) for i, s in bm25_pairs])

    # 2) Semantic retrieval (FAISS), optional
    faiss_norm: Dict[int, float] = {}
    if _STATE.faiss_index is not None and _STATE.embed_model is not None and _STATE.faiss_meta:
        try:
            import numpy as np  # type: ignore

            q_vec = _STATE.embed_model.encode([q], normalize_embeddings=True)  # type: ignore[attr-defined]
            q_vec = np.asarray(q_vec, dtype="float32")
            D, I = _STATE.faiss_index.search(q_vec, semantic_k)  # type: ignore[union-attr]
            faiss_pairs = [(int(i), float(d)) for i, d in zip(I[0].tolist(), D[0].tolist()) if int(i) >= 0]
            faiss_norm = _normalize_scores(faiss_pairs)
        except Exception:
            faiss_norm = {}

    # 3) Candidate union (by chunk index)
    candidate_is = set(bm25_norm.keys()) | set(faiss_norm.keys())
    if not candidate_is:
        return []

    # 4) Score fusion + intent soft-boost
    scored: List[Tuple[int, float]] = []
    for i in candidate_is:
        # meta lists are aligned by "i"
        if i < 0 or i >= len(_STATE.bm25_meta):
            continue
        ch = _STATE.bm25_meta[i]

        # Drop ultra-short chunks that are usually headings or fragments
        text_len = len((ch.text or "").strip())
        if text_len < 120:
            continue

        s_b = bm25_norm.get(i, 0.0)
        s_f = faiss_norm.get(i, 0.0)

        # Weighted sum (BM25 slightly preferred for legal citations / exact terms)
        score = (0.60 * s_b) + (0.40 * s_f)

        score += _soft_intent_boost(legal_object, ch)

        scored.append((i, float(score)))

    
    # 4b) Optional hard-focus filter (prevents obvious COI vs Court-Martial drift)
    if legal_object in ("Court of Inquiry", "Court-Martial"):
        def _focus_match(ch: _Chunk) -> bool:
            hp = (ch.heading_path or "").lower()
            tx = (ch.text or "").lower()
            if legal_object == "Court of Inquiry":
                return ("court of inquiry" in hp) or ("court of inquiry" in tx)
            if legal_object == "Court-Martial":
                return ("court-martial" in hp) or ("court-martial" in tx) or ("courts-martial" in tx)
            return True

        focused = [(i, s) for (i, s) in scored if _focus_match(_STATE.bm25_meta[i])]
        # Use focused set only if it is non-empty; otherwise keep the broader candidate set.
        if focused:
            scored = focused
    scored.sort(key=lambda x: x[1], reverse=True)
    top_indices = [i for i, _ in scored[: max(top_k, 1)]]

    # 5) Build citation objects with minimal context window (adjacent chunks in same file)
    out: List[Dict[str, Any]] = []
    for i in top_indices:
        ch = _STATE.bm25_meta[i]

        before = ""
        after = ""
        if i - 1 >= 0:
            prev = _STATE.bm25_meta[i - 1]
            if prev.source_file == ch.source_file:
                before = prev.text
        if i + 1 < len(_STATE.bm25_meta):
            nxt = _STATE.bm25_meta[i + 1]
            if nxt.source_file == ch.source_file:
                after = nxt.text

        heading = _short_heading(ch.heading_path)
        vol = Path(ch.source_file).stem  # "MML Vol 1"
        title = f"{vol} | {heading}"

        verbatim = (ch.text or "").strip()
        snippet = re.sub(r"\s+", " ", verbatim)[:220].strip()

        out.append(
            {
                "citation_id": ch.chunk_id,
                "document": "MML",
                "title": title,
                "source_file": ch.source_file,
                "heading": heading,
                "location": ch.heading_path,
                "verbatim": verbatim,
                "context_before": (before or "").strip(),
                "context_after": (after or "").strip(),
                "snippet": snippet,
            }
        )

    return out


# ----------------------------
# Multi-query hybrid retrieval
# ----------------------------
def retrieve_citations_multi(
    questions: List[str],
    top_k: int = 8,
    legal_object: Optional[str] = None,
    lexical_k: int = 60,
    semantic_k: int = 60,
) -> List[Dict[str, Any]]:
    """
    Multi-query hybrid retrieval with per-query fusion and cross-query max pooling.
    """
    _ensure_loaded()

    if not questions:
        return []

    # If BM25 is unavailable, we cannot retrieve reliably
    if _STATE.bm25 is None or not _STATE.bm25_meta:
        return []

    normalized_questions = [q.strip() for q in questions if (q or "").strip()]
    if not normalized_questions:
        return []

    final_scores: Dict[int, float] = {}
    hit_queries: Dict[int, List[int]] = {}

    for q_idx, q in enumerate(normalized_questions):
        # 1) Lexical retrieval (BM25)
        q_tokens = _bm25_tokenize(q)
        try:
            bm25_scores = _STATE.bm25.get_scores(q_tokens)  # type: ignore[attr-defined]
            bm25_pairs = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:lexical_k]
        except Exception:
            bm25_pairs = []

        bm25_norm = _normalize_scores([(i, float(s)) for i, s in bm25_pairs])

        # 2) Semantic retrieval (FAISS), optional
        faiss_norm: Dict[int, float] = {}
        if _STATE.faiss_index is not None and _STATE.embed_model is not None and _STATE.faiss_meta:
            try:
                import numpy as np  # type: ignore

                q_vec = _STATE.embed_model.encode([q], normalize_embeddings=True)  # type: ignore[attr-defined]
                q_vec = np.asarray(q_vec, dtype="float32")
                D, I = _STATE.faiss_index.search(q_vec, semantic_k)  # type: ignore[union-attr]
                faiss_pairs = [(int(i), float(d)) for i, d in zip(I[0].tolist(), D[0].tolist()) if int(i) >= 0]
                faiss_norm = _normalize_scores(faiss_pairs)
            except Exception:
                faiss_norm = {}

        # 3) Candidate union for this query
        candidate_is = set(bm25_norm.keys()) | set(faiss_norm.keys())
        if not candidate_is:
            continue

        for i in candidate_is:
            if i < 0 or i >= len(_STATE.bm25_meta):
                continue
            ch = _STATE.bm25_meta[i]

            # Drop ultra-short chunks that are usually headings or fragments
            text_len = len((ch.text or "").strip())
            if text_len < 120:
                continue

            s_b = bm25_norm.get(i, 0.0)
            s_f = faiss_norm.get(i, 0.0)

            fused_score = max(s_b, s_f)

            boost = 1.0 + _soft_intent_boost(legal_object, ch)
            fused_score *= boost

            prev_score = final_scores.get(i, 0.0)
            if fused_score > prev_score:
                final_scores[i] = float(fused_score)

            if fused_score > 0.0:
                hit_queries.setdefault(i, []).append(q_idx)

    if not final_scores:
        return []

    # Sort by fused score
    scored = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    top_indices = [i for i, _ in scored[: max(top_k, 1)]]

    out: List[Dict[str, Any]] = []
    for i in top_indices:
        ch = _STATE.bm25_meta[i]

        before = ""
        after = ""
        if i - 1 >= 0:
            prev = _STATE.bm25_meta[i - 1]
            if prev.source_file == ch.source_file:
                before = prev.text
        if i + 1 < len(_STATE.bm25_meta):
            nxt = _STATE.bm25_meta[i + 1]
            if nxt.source_file == ch.source_file:
                after = nxt.text

        heading = _short_heading(ch.heading_path)
        vol = Path(ch.source_file).stem
        title = f"{vol} | {heading}"

        verbatim = (ch.text or "").strip()
        snippet = re.sub(r"\s+", " ", verbatim)[:220].strip()

        out.append(
            {
                "citation_id": ch.chunk_id,
                "document": "MML",
                "title": title,
                "source_file": ch.source_file,
                "heading": heading,
                "location": ch.heading_path,
                "verbatim": verbatim,
                "context_before": (before or "").strip(),
                "context_after": (after or "").strip(),
                "snippet": snippet,
                "retrieval_score": final_scores.get(i, 0.0),
                "hit_query_count": len(hit_queries.get(i, [])),
            }
        )

    return out
