from __future__ import annotations

import logging
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import json
import pickle
import re
import threading

from apps.api.services.rerank import (
    RERANK_ENABLED,
    RERANK_TOP_N,
    rerank_candidates,
)


# ----------------------------
# Offline Hybrid Retrieval (BM25 + FAISS)
# ----------------------------
# This module is intentionally dependency-tolerant:
# - If FAISS / sentence-transformers are unavailable, it falls back to BM25 only.
# - If BM25 is unavailable, it returns an empty list (caller should handle).
#
# Artifacts expected (generated by scripts/ingest.py):
#   data/index_legal/<version>/bm25.pkl
#   data/index_legal/<version>/meta.jsonl
#   data/index_legal/<version>/faiss.index
#   data/index_legal/<version>/manifest.json
#
# Meta JSONL schema per line:
#   {"i": int, "chunk_id": str, "source_file": str, "heading_path": str, "text": str}
#
# Returned citations must include FULL verbatim text so synthesis can be grounded.
# Contract per item:
# {
#   "citation_id": "...",                # stable id used by UI and answer brackets
#   "document": "MML",
#   "title": "...",
#   "source_file": "...",
#   "heading": "...",
#   "location": "...",
#   "verbatim": "...",
#   "context_before": "...",
#   "context_after": "...",
#   "snippet": "...",
# }


@dataclass(frozen=True)
class _Chunk:
    i: int
    chunk_id: str
    source_file: str
    heading_path: str
    text: str


_TOKEN_RE = re.compile(r"[A-Za-z0-9]+(?:[-/][A-Za-z0-9]+)*")


def _bm25_tokenize(text: str) -> List[str]:
    # Match scripts/ingest.py tokenization for consistency.
    return [t.lower() for t in _TOKEN_RE.findall(text)]


def _repo_root() -> Path:
    # apps/api/services/retrieval.py -> repo root is parents[3]
    return Path(__file__).resolve().parents[3]


def _short_heading(heading_path: str) -> str:
    # heading_path format: "## CHAPTER X > ### SUBHEAD"
    parts = [p.strip() for p in heading_path.split(">") if p.strip()]
    return parts[-1] if parts else heading_path.strip()


def _soft_intent_boost(legal_object: Optional[str], chunk: _Chunk) -> float:
    """
    A small scoring boost (or penalty) to keep retrieval on the correct legal object
    without hard-filtering away potentially relevant material.
    """
    if not legal_object:
        return 0.0

    hp = (chunk.heading_path or "").lower()
    txt = (chunk.text or "").lower()

    if legal_object == "Court of Inquiry":
        # Strong positive if evidence explicitly mentions COI
        if "court of inquiry" in hp or "court of inquiry" in txt:
            return 0.20
        # Penalize court-martial dominant content unless COI appears too
        if ("court-martial" in hp or "court-martial" in txt) and "court of inquiry" not in txt:
            return -0.15
        return 0.0

    if legal_object == "Court-Martial":
        if "court-martial" in hp or "court-martial" in txt or "courts-martial" in txt:
            return 0.20
        if "court of inquiry" in hp or "court of inquiry" in txt:
            return -0.10
        return 0.0

    if legal_object == "Disciplinary Action":
        if any(k in txt for k in ("awl", "awol", "absence without leave", "desertion")):
            return 0.15
        return 0.0

    return 0.0


def _focus_terms_for_object(legal_object: Optional[str]) -> List[str]:
    """Return focus terms based on the detected legal object."""
    if not legal_object:
        return []

    lo = legal_object.lower()
    if "court of inquiry" in lo or "coi" == lo:
        return ["court of inquiry", "coi"]
    if "court-martial" in lo or "court martial" in lo:
        return ["court-martial", "court martial"]
    if "disciplinary" in lo:
        return ["disciplinary", "charge", "punishment"]
    return []


class _RetrievalState:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self.loaded = False

        self.index_version: Optional[str] = None
        self.manifest: Dict[str, Any] = {}

        self.bm25 = None
        self.bm25_meta: List[_Chunk] = []

        self.faiss_index = None
        self.faiss_meta: List[_Chunk] = []
        self.embed_model = None

        self.load_errors: List[str] = []


_STATE = _RetrievalState()


logger = logging.getLogger(__name__)


def _load_meta_jsonl(path: Path) -> List[_Chunk]:
    chunks: List[_Chunk] = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            chunks.append(
                _Chunk(
                    i=int(obj["i"]),
                    chunk_id=str(obj["chunk_id"]),
                    source_file=str(obj["source_file"]),
                    heading_path=str(obj.get("heading_path", "")),
                    text=str(obj.get("text", "")),
                )
            )
    return chunks


def _resolve_index_dir(base_dir: Path) -> Tuple[Path, str]:
    if not base_dir.exists():
        raise RuntimeError(
            f"Legal index directory not found at {base_dir.resolve()}. Run `python scripts/ingest.py` first."
        )

    env_version = os.getenv("LEGAL_INDEX_VERSION")
    if env_version:
        candidate = base_dir / env_version
        if not candidate.is_dir():
            raise RuntimeError(
                f"LEGAL_INDEX_VERSION set to '{env_version}', but {candidate} does not exist. Run ingest or adjust the version."
            )
        return candidate, env_version

    candidates = sorted([p for p in base_dir.iterdir() if p.is_dir()], key=lambda p: p.name)
    if not candidates:
        raise RuntimeError(
            f"No legal index versions found under {base_dir.resolve()}. Run `python scripts/ingest.py` first."
        )
    version_dir = candidates[-1]
    return version_dir, version_dir.name


def _load_manifest(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _ensure_loaded() -> None:
    if _STATE.loaded:
        return
    with _STATE._lock:
        if _STATE.loaded:
            return

        root = _repo_root()
        index_base = root / "data" / "index_legal"

        version_dir, version_name = _resolve_index_dir(index_base)
        manifest_path = version_dir / "manifest.json"
        meta_path = version_dir / "meta.jsonl"

        _STATE.index_version = version_name
        _STATE.manifest = _load_manifest(manifest_path)

        # BM25 (required)
        try:
            with (version_dir / "bm25.pkl").open("rb") as f:
                _STATE.bm25 = pickle.load(f)
            _STATE.bm25_meta = _load_meta_jsonl(meta_path)
        except Exception as e:
            _STATE.load_errors.append(f"BM25 load failed: {e}")
            _STATE.bm25 = None
            _STATE.bm25_meta = []

        # FAISS + embeddings (optional)
        try:
            import faiss  # type: ignore
            from sentence_transformers import SentenceTransformer  # type: ignore

            _STATE.faiss_index = faiss.read_index(str(version_dir / "faiss.index"))
            _STATE.faiss_meta = _load_meta_jsonl(meta_path)

            model_name = _STATE.manifest.get("embedding_model") or "sentence-transformers/all-MiniLM-L6-v2"
            _STATE.embed_model = SentenceTransformer(model_name)
        except Exception as e:
            _STATE.load_errors.append(f"FAISS load failed (fallback to BM25-only): {e}")
            _STATE.faiss_index = None
            _STATE.faiss_meta = []
            _STATE.embed_model = None

        _STATE.loaded = True


def get_legal_index_info() -> Dict[str, Any]:
    """Return the loaded legal index version and manifest."""
    _ensure_loaded()
    return {"version": _STATE.index_version, "manifest": dict(_STATE.manifest)}


def _normalize_scores(pairs: List[Tuple[int, float]]) -> Dict[int, float]:
    if not pairs:
        return {}
    scores = [s for _, s in pairs]
    hi = max(scores)
    lo = min(scores)
    out: Dict[int, float] = {}
    if hi == lo:
        for i, _s in pairs:
            out[i] = 1.0
        return out
    for i, s in pairs:
        out[i] = (s - lo) / (hi - lo)
    return out


# ----------------------------
# Single-query hybrid retrieval
# ----------------------------
def retrieve_citations(
    question: str,
    top_k: int = 8,
    legal_object: Optional[str] = None,
    lexical_k: int = 60,
    semantic_k: int = 60,
) -> List[Dict[str, Any]]:
    """
    Hybrid retrieval over MML chunks.

    - legal_object: optional intent constraint used for soft scoring.
    - Returns top_k citations with full verbatim text and minimal context.
    """
    _ensure_loaded()

    q = (question or "").strip()
    if not q:
        return []

    # If BM25 is unavailable, we cannot retrieve reliably
    if _STATE.bm25 is None or not _STATE.bm25_meta:
        return []

    # 1) Lexical retrieval (BM25)
    q_tokens = _bm25_tokenize(q)
    try:
        bm25_scores = _STATE.bm25.get_scores(q_tokens)  # type: ignore[attr-defined]
        # top lexical_k indices by score
        bm25_pairs = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:lexical_k]
    except Exception:
        bm25_pairs = []

    bm25_norm = _normalize_scores([(i, float(s)) for i, s in bm25_pairs])

    # 2) Semantic retrieval (FAISS), optional
    faiss_norm: Dict[int, float] = {}
    if _STATE.faiss_index is not None and _STATE.embed_model is not None and _STATE.faiss_meta:
        try:
            import numpy as np  # type: ignore

            q_vec = _STATE.embed_model.encode([q], normalize_embeddings=True)  # type: ignore[attr-defined]
            q_vec = np.asarray(q_vec, dtype="float32")
            D, I = _STATE.faiss_index.search(q_vec, semantic_k)  # type: ignore[union-attr]
            faiss_pairs = [(int(i), float(d)) for i, d in zip(I[0].tolist(), D[0].tolist()) if int(i) >= 0]
            faiss_norm = _normalize_scores(faiss_pairs)
        except Exception:
            faiss_norm = {}

    # 3) Candidate union (by chunk index)
    candidate_is = set(bm25_norm.keys()) | set(faiss_norm.keys())
    if not candidate_is:
        return []

    # 4) Score fusion + intent soft-boost
    scored: List[Tuple[int, float]] = []
    for i in candidate_is:
        # meta lists are aligned by "i"
        if i < 0 or i >= len(_STATE.bm25_meta):
            continue
        ch = _STATE.bm25_meta[i]

        # Drop ultra-short chunks that are usually headings or fragments
        text_len = len((ch.text or "").strip())
        if text_len < 120:
            continue

        s_b = bm25_norm.get(i, 0.0)
        s_f = faiss_norm.get(i, 0.0)

        # Weighted sum (BM25 slightly preferred for legal citations / exact terms)
        score = (0.60 * s_b) + (0.40 * s_f)

        score += _soft_intent_boost(legal_object, ch)

        scored.append((i, float(score)))

    
    # 4b) Optional hard-focus filter (prevents obvious COI vs Court-Martial drift)
    if legal_object in ("Court of Inquiry", "Court-Martial"):
        def _focus_match(ch: _Chunk) -> bool:
            hp = (ch.heading_path or "").lower()
            tx = (ch.text or "").lower()
            if legal_object == "Court of Inquiry":
                return ("court of inquiry" in hp) or ("court of inquiry" in tx)
            if legal_object == "Court-Martial":
                return ("court-martial" in hp) or ("court-martial" in tx) or ("courts-martial" in tx)
            return True

        focused = [(i, s) for (i, s) in scored if _focus_match(_STATE.bm25_meta[i])]
        # Use focused set only if it is non-empty; otherwise keep the broader candidate set.
        if focused:
            scored = focused
    scored.sort(key=lambda x: x[1], reverse=True)
    top_indices = [i for i, _ in scored[: max(top_k, 1)]]

    # 5) Build citation objects with minimal context window (adjacent chunks in same file)
    out: List[Dict[str, Any]] = []
    for i in top_indices:
        ch = _STATE.bm25_meta[i]

        before = ""
        after = ""
        if i - 1 >= 0:
            prev = _STATE.bm25_meta[i - 1]
            if prev.source_file == ch.source_file:
                before = prev.text
        if i + 1 < len(_STATE.bm25_meta):
            nxt = _STATE.bm25_meta[i + 1]
            if nxt.source_file == ch.source_file:
                after = nxt.text

        heading = _short_heading(ch.heading_path)
        vol = Path(ch.source_file).stem  # "MML Vol 1"
        title = f"{vol} | {heading}"

        verbatim = (ch.text or "").strip()
        snippet = re.sub(r"\s+", " ", verbatim)[:220].strip()

        out.append(
            {
                "citation_id": ch.chunk_id,
                "document": "MML",
                "title": title,
                "source_file": ch.source_file,
                "heading": heading,
                "location": ch.heading_path,
                "verbatim": verbatim,
                "context_before": (before or "").strip(),
                "context_after": (after or "").strip(),
                "snippet": snippet,
            }
        )

    return out


# ----------------------------
# Multi-query hybrid retrieval
# ----------------------------
def retrieve_citations_multi(
    questions: List[str],
    top_k: int = 8,
    legal_object: Optional[str] = None,
    lexical_k: int = 60,
    semantic_k: int = 60,
) -> List[Dict[str, Any]]:
    """
    Multi-query hybrid retrieval with per-query fusion and cross-query max pooling.
    """
    _ensure_loaded()

    if not questions:
        return []

    # If BM25 is unavailable, we cannot retrieve reliably
    if _STATE.bm25 is None or not _STATE.bm25_meta:
        return []

    normalized_questions = [q.strip() for q in questions if (q or "").strip()]
    if not normalized_questions:
        return []

    final_scores: Dict[int, float] = {}
    hit_queries: Dict[int, List[int]] = {}

    for q_idx, q in enumerate(normalized_questions):
        # 1) Lexical retrieval (BM25)
        q_tokens = _bm25_tokenize(q)
        try:
            bm25_scores = _STATE.bm25.get_scores(q_tokens)  # type: ignore[attr-defined]
            bm25_pairs = sorted(enumerate(bm25_scores), key=lambda x: x[1], reverse=True)[:lexical_k]
        except Exception:
            bm25_pairs = []

        bm25_norm = _normalize_scores([(i, float(s)) for i, s in bm25_pairs])

        # 2) Semantic retrieval (FAISS), optional
        faiss_norm: Dict[int, float] = {}
        if _STATE.faiss_index is not None and _STATE.embed_model is not None and _STATE.faiss_meta:
            try:
                import numpy as np  # type: ignore

                q_vec = _STATE.embed_model.encode([q], normalize_embeddings=True)  # type: ignore[attr-defined]
                q_vec = np.asarray(q_vec, dtype="float32")
                D, I = _STATE.faiss_index.search(q_vec, semantic_k)  # type: ignore[union-attr]
                faiss_pairs = [(int(i), float(d)) for i, d in zip(I[0].tolist(), D[0].tolist()) if int(i) >= 0]
                faiss_norm = _normalize_scores(faiss_pairs)
            except Exception:
                faiss_norm = {}

        # 3) Candidate union for this query
        candidate_is = set(bm25_norm.keys()) | set(faiss_norm.keys())
        if not candidate_is:
            continue

        for i in candidate_is:
            if i < 0 or i >= len(_STATE.bm25_meta):
                continue
            ch = _STATE.bm25_meta[i]

            # Drop ultra-short chunks that are usually headings or fragments
            text_len = len((ch.text or "").strip())
            if text_len < 120:
                continue

            s_b = bm25_norm.get(i, 0.0)
            s_f = faiss_norm.get(i, 0.0)

            fused_score = max(s_b, s_f)

            boost = 1.0 + _soft_intent_boost(legal_object, ch)
            fused_score *= boost

            prev_score = final_scores.get(i, 0.0)
            if fused_score > prev_score:
                final_scores[i] = float(fused_score)

            if fused_score > 0.0:
                hit_queries.setdefault(i, []).append(q_idx)

    if not final_scores:
        return []
    # Sort by fused score (retrieval ranking)
    scored = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)

    # Build candidate set with retrieval metadata
    candidates_all: List[Dict[str, Any]] = []
    for i, score in scored:
        ch = _STATE.bm25_meta[i]
        candidates_all.append(
            {
                "chunk_index": i,
                "chunk": ch,
                "heading_path": ch.heading_path,
                "text": ch.text,
                "source_file": ch.source_file,
                "retrieval_score": float(score),
                "hit_query_count": len(hit_queries.get(i, [])),
            }
        )

    # ----------------------------
    # Conditional focus filtering
    # ----------------------------
    focus_terms = _focus_terms_for_object(legal_object)
    candidates_focus: List[Dict[str, Any]] = []
    if focus_terms:
        for cand in candidates_all:
            hp = (cand.get("heading_path") or "").lower()
            txt = (cand.get("text") or "").lower()
            if any(term in hp or term in txt for term in focus_terms):
                candidates_focus.append(cand)

    candidates_for_ranking = candidates_all
    focus_applied = False
    if focus_terms and len(candidates_focus) >= top_k * 5:
        candidates_for_ranking = candidates_focus
        focus_applied = True

    logger.info(
        "retrieval.focus_filter applied=%s before=%d after=%d",
        focus_applied,
        len(candidates_all),
        len(candidates_for_ranking),
    )

    # ----------------------------
    # Cross-encoder reranking
    # ----------------------------
    rerank_top_n = max(1, RERANK_TOP_N)
    rerank_candidates_input = candidates_for_ranking[:rerank_top_n]
    rerank_start = time.perf_counter()
    reranked = rerank_candidates(
        question=normalized_questions[0],
        candidates=rerank_candidates_input,
        top_k=top_k,
    )
    rerank_latency_ms = (time.perf_counter() - rerank_start) * 1000.0
    logger.info(
        "retrieval.rerank rerank_enabled=%s rerank_top_n=%d latency_ms=%.2f",
        RERANK_ENABLED,
        rerank_top_n,
        rerank_latency_ms,
    )

    if not reranked:
        reranked = candidates_for_ranking[:top_k]

    final_candidates = reranked

    out: List[Dict[str, Any]] = []
    for cand in final_candidates:
        ch = cand.get("chunk") or _STATE.bm25_meta[cand.get("chunk_index", 0)]
        i = cand.get("chunk_index", 0)

        before = ""
        after = ""
        if i - 1 >= 0:
            prev = _STATE.bm25_meta[i - 1]
            if prev.source_file == ch.source_file:
                before = prev.text
        if i + 1 < len(_STATE.bm25_meta):
            nxt = _STATE.bm25_meta[i + 1]
            if nxt.source_file == ch.source_file:
                after = nxt.text

        heading = _short_heading(ch.heading_path)
        vol = Path(ch.source_file).stem
        title = f"{vol} | {heading}"

        verbatim = (ch.text or "").strip()
        snippet = re.sub(r"\s+", " ", verbatim)[:220].strip()

        out.append(
            {
                "citation_id": ch.chunk_id,
                "document": "MML",
                "title": title,
                "source_file": ch.source_file,
                "heading": heading,
                "location": ch.heading_path,
                "verbatim": verbatim,
                "context_before": (before or "").strip(),
                "context_after": (after or "").strip(),
                "snippet": snippet,
                "retrieval_score": cand.get("retrieval_score", 0.0),
                "hit_query_count": cand.get("hit_query_count", 0),
                "focus_applied": focus_applied,
                "rerank_score": cand.get("rerank_score"),
            }
        )

    return out
